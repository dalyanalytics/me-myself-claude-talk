---
title: "Me, Myself & Claude"
subtitle: "How I Leverage AI-Assisted Development to Scale My Solo R-based Data Science Consultancy"
author: "Jasmine Daly<br>Principal Consultant & Founder<br>Daly Analytics"
date: "Wednesday, November 12, 2025"
format:
  revealjs:
    theme: [simple, theme/daly-analytics.scss]
    transition: slide
    slide-number: true
    chalkboard: true
    preview-links: auto
    code-line-numbers: false
    highlight-style: github
    logo: images/sunset.png
---

## Introduction {.center}

**AI-Assisted Development in R**

A journey of staying in your lane while scaling your impact!

::: {.notes}
Hello everyone! Today I want to share my experience integrating AI into my R development workflow - not as a replacement for expertise, but as a strategic force multiplier.
:::

---

## Who Am I? {.smaller}

- R Developer & Data Scientist since 2014
- Maintainer of {shinyLP}, {ttbbeer}, {avilistr}^, {shinyfa}^
- Running a *mostly* solo consultancy focused on delivering solutions with data science, analytics & AI 
- Building Shiny applications, interactive dashboards, and data automation pipelines


::: {.notes}
To level-set everyone and provide some context: I'm a mom of 2 young kids with limited time resources. I'm also an active Volunteer in my local community.

I'm managing both the technical (writing code) and business sides (doing discovery/sales calls) of my practice. I just recently onboarded a subcontractor this Summer that provides adhoc shiny development support, which has been wonderful in delegating tasks to them. 
:::

---

## üîë Moving Beyond 'Vibe Coding' {.smaller}

**Words matter. Let's reframe the conversation.**

::: {.incremental}
- **"Vibe Coding"** ‚Üí Implies reckless, hope-based development
- **"Vibe Engineering"** ‚Üí Intentional, accountable AI-assisted development
- **"I'm adding a new (agentic) tool to my tool box"** ‚Üí Honest acknowledgment of exploration of this phase of hypergrowth and learning
:::

::: {.fragment}
**The Reality:**

We're in an **exploratory phase**. There are no settled best practices yet. Making mistakes is part of professional growth.

But you remain **fully accountable** for the outcome.
:::

::: {.notes}
Credit to Christopher Fitkin's LinkedIn post on "vibe engineering" which really resonated with me.

One of the key takeaways of this entire talk is to reframe our language around how we talk about AI-assisted coding. "Vibe Coding" - while often funny - really diminishes our impact and the capacity for others to understand our lived experience as software developers.

It also does a disservice to our community when it comes to advocating for democratizing our field for diverse participants while also trying to emphasize the real complexity around ethics, governance, and professional responsibility.

The distinction Fitkin makes is critical: "vibe coding" suggests careless dependency where you hope AI outputs work. "Vibe engineering" means experienced developers intentionally leveraging AI tools while maintaining full accountability.

This isn't about replacing human expertise - it's about experienced engineers guiding AI tool adoption as an enhancement to craft, not a substitute for professional judgment. 

tl;dr: it's hard to say things are easy, give people the confidence and tools and then back track and say 'well that's not serious data science" so everything you 'vibe coded' shouldnt be used.
:::

# Part 1: Strategic Thinking Unlocked for Greater Community Contributions

## The Mental Bandwidth Shift {.smaller}

<div style="display: flex; justify-content: center; align-items: center; gap: 40px; padding: 20px 10px; max-width: 100%; margin: 0 auto;">

<div style="text-align: center; flex: 1; max-width: 280px;">
<h3 style="font-size: 1.6em; margin-bottom: 15px; color: #404041; font-weight: bold;">Before AI</h3>

<div style="background: #6A94C9; height: 340px; display: flex; flex-direction: column; justify-content: center; margin-bottom: 8px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.2em; font-weight: bold; color: white; line-height: 1.3;">Client Dev</div>
<div style="font-size: 2em; font-weight: bold; color: white; margin-top: 5px;">85%</div>
</div>

<div style="background: #D68A93; height: 60px; display: flex; flex-direction: column; justify-content: center; margin-bottom: 8px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); overflow: visible;">
<div style="font-size: 0.95em; font-weight: bold; color: white; line-height: 1.2;">Biz Dev 15%</div>
</div>
</div>

<div style="text-align: center; flex: 0 0 auto;">
<div style="font-size: 4em; color: #F9B397; font-weight: bold; line-height: 1;">‚Üí</div>
<div style="font-size: 1em; color: #404041; font-weight: bold; margin-top: 5px; line-height: 1.2;">AI Unlocks<br>15%</div>
</div>

<div style="text-align: center; flex: 1; max-width: 280px;">
<h3 style="font-size: 1.6em; margin-bottom: 15px; color: #404041; font-weight: bold;">With AI</h3>

<div style="background: #6A94C9; height: 280px; display: flex; flex-direction: column; justify-content: center; margin-bottom: 8px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.2em; font-weight: bold; color: white; line-height: 1.3;">Client Dev</div>
<div style="font-size: 2em; font-weight: bold; color: white; margin-top: 5px;">70%</div>
</div>

<div style="background: #D68A93; height: 100px; display: flex; flex-direction: column; justify-content: center; margin-bottom: 8px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1em; font-weight: bold; color: white; line-height: 1.2;">Biz Dev</div>
<div style="font-size: 1.6em; font-weight: bold; color: white; margin-top: 3px;">25%</div>
</div>

<div style="background: #AD92B1; min-height: 40px; height: 40px; display: flex; flex-direction: column; justify-content: center; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); padding: 5px 10px;">
<div style="font-size: 0.85em; font-weight: bold; color: white; line-height: 1.1;">Community 5%</div>
</div>
</div>

</div>

---


## What Mental Bandwidth Buys You

::: {.incremental}
- **Better Biz Ops:** Improving my systems to be automated and Standardized
- **Financial Clarity:** Time to understand cash flow, pricing strategy, and profitability
- **Relationship Building:** Networking, content creation, community engagement
- **Creative Space:** Room to think about what *could* be built, not just what *must* be built
:::

::: {.notes}
The mental bandwidth shift from 85% client dev to 70% client dev freed up 15% of my cognitive capacity. That 15% doesn't sound like much, but it's transformational.

**Better Biz Ops:** I now have time to build automated systems and standardized processes. Proposals, contracts, invoicing - things that used to be rushed now have proper templates and workflows.

**Financial Clarity:** Understanding cash flow, pricing strategy, and profitability isn't a "someday" task anymore. I can make strategic financial decisions instead of reactive ones.

**Relationship Building:** Networking isn't just attending events - it's consistent content creation, community engagement, and showing up in ways that build long-term relationships.

**Creative Space:** This is the big one. I now have room to think about what *could* be built, not just what *must* be built. That's where packages like {shinyfa} come from - tapping into real and persistent problems and creative exploration that serves the community.

Before AI: I was always feeling behind on the business side, constantly in reactive mode.

After AI: More strategic, more sustainable, and more creative. I'm building a business, not just executing projects.
:::

---

## The {shinyfa} Package {.smaller}

<div style="text-align: center; margin-bottom: 20px;">
  <img src="images/shinyfa.png" width="120" style="display: inline-block;">
</div>

<div style="display: flex; justify-content: center; align-items: center; gap: 20px; margin: 30px auto; max-width: 100%;">

<div style="background: #6A94C9; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">Client Problem</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">Months to navigate large Shiny app</div>
</div>

<div style="font-size: 2.5em; color: #F9B397; font-weight: bold;">‚Üí</div>

<div style="background: #AD92B1; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">Quick Script</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">Analyze file structure</div>
</div>

<div style="font-size: 2.5em; color: #F9B397; font-weight: bold;">‚Üí</div>

<div style="background: #D68A93; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">CRAN Package</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">Community tool</div>
</div>

</div>

::: {.fragment}
<div style="text-align: center; margin-top: 30px;">
**Before AI:** "That would be nice someday... but I have client work."

**With AI:** Built and released in weeks, not shelved as "someday."
</div>
:::

::: {.notes}
This is a perfect example of how AI freed up mental bandwidth for creative work.

**The Client Problem:** I was contracted to work on a large, complex Shiny app for a conservation nonprofit. It took me MONTHS to get comfortable in their repo - understanding which files did what, how the reactive dependencies flowed, where inputs and outputs were defined. It was a significant onboarding challenge.

**The Insight:** About halfway through the project, I had this realization: "I'm a Shiny developer. There has to be a better way to systematically analyze app structure." So I wrote a one-off script to help me catalog the render functions, reactive functions, inputs, and file relationships in their app.

**The Evolution:** That script was so useful that I kept refining it. Then I thought - "Other developers face this same problem." With Claude's help, I generalized those functions, added proper documentation, built tests, and packaged it up as {shinyfa}.

**The Timeline:** The entire process from "useful script" to "published package" took weeks, not months. Without AI assistance, this would have stayed as a one-off script on my hard drive, or been a "someday" project I never got to.

**The Impact:** This is what mental bandwidth buys you: the ability to take a client problem, generalize it, and contribute a tool back to the community - all without sacrificing your business. {shinyfa} solves a problem I experienced repeatedly - onboarding developers to complex Shiny apps. It would have saved me weeks during that client engagement.
:::

---

## {shinyfa} in Action {.smaller}

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "|4|7-10|13|16"

library(shinyfa)
library(dplyr)

file_path_df <- list.files("SHINY-SERVER-DIRECTORY",
                           pattern = "\\.R$", full.names = TRUE)

file_analysis <- data.frame()

for (file in file_path_df) {
  shiny_analysis <- analyze_shiny_reactivity(file_path = file)

  if (is.null(shiny_analysis)) next  # Skip empty files

  shiny_analysis$file_name <- basename(file)

  file_analysis <- bind_rows(file_analysis, shiny_analysis)
}

print(file_analysis)
```

**Result:** Catalog of render functions, reactive functions, inputs, and file relationships across your entire Shiny app.

::: {.notes}
This is the core workflow for using {shinyfa}. You point it at your Shiny app directory, and it systematically analyzes each R file.

The `analyze_shiny_reactivity()` function is doing the heavy lifting - it parses each file and extracts information about reactivity patterns. The result is a dataframe that gives you a complete map of your app's structure.

This is exactly what I needed during that conservation nonprofit project. Instead of manually clicking through dozens of files trying to understand "where is this input used?" or "which files have render functions?", you get a structured dataset you can query, filter, and explore.

The code is straightforward R - iterate through files, analyze each one, bind results. But the value is enormous when you're trying to onboard to a complex codebase or understand the architecture of a large Shiny app.
:::

---

## {shinyfa} Output Example {.smaller}

```{r}
#| echo: false
#| eval: false

# Example output from analyze_shiny_reactivity()
file_analysis
```

| file_name | type | name | input_id | output_id |
|-----------|------|------|----------|-----------|
| ui.R | input | selectInput | "species" | NA |
| ui.R | output | plotOutput | NA | "dist_plot" |
| server.R | reactive | filtered_data | NA | NA |
| server.R | render | renderPlot | "species" | "dist_plot" |
| modules/map.R | input | sliderInput | "zoom" | NA |
| modules/map.R | render | renderLeaflet | "zoom" | "map" |


::: {.notes}
This is what the output dataframe looks like after running analyze_shiny_reactivity() across your Shiny app files.

Each row represents a reactive element in your app: inputs, outputs, reactive expressions, and render functions. You can see which file they're in, what type they are, their name, and how they connect (input_id, output_id).

For that conservation nonprofit project, this would have been transformational during onboarding. Instead of searching through files asking "where is the species input used?", I could just filter this dataframe: file_analysis %>% filter(input_id == "species") and instantly see everywhere it's referenced.

The real power is in queries like:
- "Show me all render functions in the modules folder"
- "Which files have reactive expressions?"
- "What outputs don't have corresponding render functions?" (potential bugs!)
- "Map the flow from this input to its output"

This structured view of your app's reactivity is what makes {shinyfa} useful for onboarding, documentation, and understanding complex Shiny applications.
:::

---

## The {avilistr} Package {.smaller}

<div style="display: flex; justify-content: center; align-items: center; gap: 20px; margin: 30px auto; max-width: 100%;">

<div style="background: #AD92B1; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">New Hobby</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">Started birding, found taxonomy in Excel</div>
</div>

<div style="font-size: 2.5em; color: #F9B397; font-weight: bold;">‚Üí</div>

<div style="background: #6A94C9; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">Wrap Excel</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">11k+ species, Cornell Lab codes</div>
</div>

<div style="font-size: 2.5em; color: #F9B397; font-weight: bold;">‚Üí</div>

<div style="background: #D68A93; padding: 15px 20px; border-radius: 8px; flex: 1; max-width: 180px; text-align: center; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
<div style="font-size: 1.1em; font-weight: bold; color: white; line-height: 1.3;">CRAN Package</div>
<div style="font-size: 0.9em; color: white; margin-top: 8px;">In a weekend</div>
</div>

</div>

::: {.fragment}
<div style="text-align: center; margin-top: 30px;">
**Before AI:** "I'd love to do this, but this feels like a nice to have and not crucial."

**With AI:** Published with comprehensive docs and vignettes in a weekend.
</div>
:::

::: {.notes}
**The Personal Story:** I started birding in 2025 - a new hobby outside of tech. I discovered the AviList Global Avian Checklist, which provides a unified global bird taxonomy. The problem? It was in Excel. For R users who want to work with bird data - especially integrating with eBird through the {rebird} package - having this taxonomy accessible in R makes workflows much cleaner.

**The Technical Reality:** Total honesty - this wasn't some groundbreaking technical achievement. I took a published Excel sheet and wrapped it into an R package. The Excel conversion itself was straightforward, not a heavy lift at all.

**Where AI Made the Difference:** The real work was creating the starter code and documentation. The getting-started vignette shows practical workflows: exploring the 11,131 species, finding specific groups (like all 194 thrush species), comparing naming differences between taxonomies, and connecting to real-time eBird observations.

**The Timeline:** With AI assistance, I could focus on "what examples would actually help birders?" rather than getting bogged down in the tedium of writing documentation. Claude helped me structure the vignettes, write clear examples, and make the package immediately useful. The entire process - from idea to published on CRAN with comprehensive documentation - took less than a week.

**The Bigger Message:** This is about lowering the barrier to contribution. You don't need to build something technically complex to add value to a community. Sometimes it's just "take this useful thing in the wrong format and make it accessible to your community" - and with AI, doing that well (with good docs!) becomes feasible in days, not months.

Finding common ground in open source: birding + R + making things accessible = a contribution I'm genuinely proud of.
:::

---

## {avilistr} in Action {.smaller}

```{r}
#| echo: true
#| eval: false

library(avilistr)
library(dplyr)

# Load the data
data(avilist_2025)

# How many species in each bird family?
family_counts <- avilist_2025 %>%
  filter(Taxon_rank == "species") %>%
  count(Family, sort = TRUE) %>%
  head(10)

family_counts
```

**Top 10 Bird Families:**
- Tyrannidae (Tyrant Flycatchers): 441 species
- Thraupidae (Tanagers): 390 species
- Trochilidae (Hummingbirds): 363 species
- Muscicapidae (Old World Flycatchers): 352 species
- Columbidae (Pigeons and Doves): 350 species

::: {.notes}
This is a simple example showing how birders can explore the taxonomy data. The `avilist_2025` dataset is immediately available when you load the package.

With just a few lines of dplyr code, you can answer questions like "Which bird families are the most diverse?" This kind of exploration would be tedious with an Excel file but is straightforward with the data in R.

The package includes Cornell Lab species codes, so you can easily integrate with eBird data through the {rebird} package to get real-time observations of species from these families.
:::

---

## {avilistr} Output Example {.smaller}

```{r}
#| echo: false
#| eval: true
#| code-line-numbers: "|3-5|8-12"

library(ggplot2)
library(dplyr)
library(avilistr)

# Compare species counts across major bird orders
data(avilist_2025)

order_diversity <- avilist_2025 %>%
  filter(Taxon_rank == "species") %>%
  count(Order, sort = TRUE) %>%
  filter(n > 100)

ggplot(order_diversity, aes(x = reorder(Order, n), y = n)) +
  geom_col(fill = "#6A94C9") +
  coord_flip() +
  labs(title = "Bird Diversity by Order (>100 species)",
       x = NULL, y = "Number of Species") +
  theme_minimal()
```

::: {.fragment}
**Passeriformes dominates** with 6,700+ species - over half of all bird species!
:::

::: {.notes}
This shows a simple visualization you can create with the avilistr data. The code is straightforward - group by taxonomic order, count species, and plot.

What's interesting is discovering patterns like Passeriformes (perching birds/songbirds) representing over 60% of all bird species. This kind of exploratory analysis is exactly what R birders want to do.

The value isn't in complex data processing - it's in making the taxonomy accessible so birders can ask and answer their own questions. With AI help, I could focus on creating these kinds of useful examples in the documentation rather than struggling with package structure and CRAN submission requirements.
:::

---

## The Creative Multiplier

AI assistance doesn't just save time on what you were already doing.

**It unlocks what you weren't doing at all:**

- Open source contributions (like shinyfa and avilistr)
- Writing better tests and documentation
- Building developer tools that solve real problems
- Package development with proper structure
- Community engagement and knowledge sharing

::: {.notes}
*Side note: ChatGPT is great for hex sticker design and branding ideas!*
:::

::: {.notes}
{ttbbeer} my first package was released July 2016 and 
{shinyLP} my second package was first released on September 2016. The next CRAN package i released was {avilistr} in June 2025. That's 8 years 9 months (September 16, 2016‚Äâ‚Äì‚ÄâJune 17, 2025)
:::


# Part 2: Saying Yes to Bigger & Funner Problems

## Optimizing for Professional Joy

**As a solo consultant, I'm not optimizing for maximum revenue‚ÄîI'm optimizing for professional joy.**

::: {.incremental}
**Professional joy means:**

- **Values-aligned clients:** Nonprofits, social impact organizations, NGOs
- **High personal interaction:** Deep relationships, not transactional projects
- **Quality over quantity:** Maintaining high standards for my client base
- **Technical depth:** Challenging problems that expand my R expertise
- **Professional development:** Time for community engagement, learning, conferences
- **Life balance:** Not sacrificing home life for business growth
:::

::: {.fragment}
**AI assistance doesn't just free up time‚Äîit lets me be selective about where I invest it.**
:::

::: {.notes}
This is the core philosophy that drives everything in Part 2. I'm running a solo consultancy, and I could optimize for different things‚Äîmaximum revenue, maximum number of clients, or rapid business growth. But that's not what I'm after.

**Professional Joy as the North Star:** I'm optimizing for professional joy, which for me means working with clients whose missions I care about. Nonprofits, social impact organizations, NGOs‚Äîthese are organizations doing important work in the world, often with limited resources. I find deep satisfaction in helping them leverage data and technology to amplify their impact.

**High Personal Interaction:** I'm not interested in being a code factory churning out anonymous projects. I want deep relationships with my clients‚Äîunderstanding their challenges, being a trusted partner, and seeing the long-term impact of the work we do together.

**Quality Over Quantity:** Maintaining high standards means being selective. Not every project is a good fit, even if the money is good. I'd rather work with 5 clients I'm deeply invested in than 20 clients where I'm just checking boxes.

**Technical Depth:** Professional joy also means working on problems that challenge me technically. I want to keep growing as an R developer, learning new packages, exploring new approaches, and building expertise in areas like data engineering, automation, and infrastructure.

**Professional Development:** This includes speaking at conferences like this one, contributing open source packages, participating in the R community, and continuous learning. These activities don't directly generate revenue, but they're essential to professional satisfaction and long-term growth.

**Life Balance:** I'm a solo consultant, not a startup founder trying to build a unicorn. I have a life outside of work that matters‚Äîfamily, hobbies, health. Sustainable consulting means not burning out.

**The AI Advantage:** Here's where AI becomes transformative. It doesn't just save me time‚Äîit gives me the capacity to be *selective*. I can say YES to the projects that bring professional joy and NO to the ones that don't, because AI assistance means I'm not constantly capacity-constrained. I can take on challenging technical problems without sacrificing quality or work-life balance.

This philosophy sets up everything that follows in Part 2. The technical skills and workflows I'm about to share aren't about doing more work‚Äîthey're about doing the *right* work, with the *right* clients, in a way that's sustainable and joyful.
:::

---

## The Growth Question

Before AI-assisted development, my filter for new projects was:

> "Can I build this in a reasonable timeframe with my current skillset given my existing client workload?"

::: {.notes}
**This often meant saying NO to:**

- Projects requiring unfamiliar R packages 
- Learning new techniques to support the project
- Interesting client opportunities
:::
---

## The New Filter

With AI-assisted development, the question became:

> "Can I can deeply understand the desired outcome based on the contraints? I'm pretty sure I can build this"

::: {.notes}
**Now I can say YES to:**

- Pipeline development and automation
- Data engineering workflows
- Complex hosting solutions
- Technologies outside my comfort zone but within my skillset
:::
---

## Claude Code as My Coding Partner {.smaller}

**The Mental Model:**

::: {.incremental}
- **You are the manager** - Setting direction, making architectural decisions
- **Claude is your senior developer** - Implementing solutions, handling boilerplate
- **Human in the loop** - Technical leadership stays with you
:::

::: {.fragment}
This isn't about replacing your skills‚Äîit's about **amplifying your capacity** to take on bigger, more interesting problems.
:::

::: {.notes}
This is the critical mental model shift. You're not becoming dependent on AI or losing your technical skills. You're establishing a working relationship where you maintain technical leadership.

**You are the manager:** You understand the client requirements, make architectural decisions, choose the technology stack, and own the final deliverable.

**Claude is your senior developer:** It handles implementation details, writes boilerplate, suggests approaches, and speeds up the coding process.

**Human in the loop:** You review everything, test rigorously, and course-correct when Claude goes off track. Your technical judgment is what ensures quality.

This model lets you say YES to bigger projects while maintaining the quality and technical ownership that makes you valuable to clients.
:::

---

## Example: GitHub Actions Data Orchestration {.smaller}

**The Client Challenge:**

- Multi-client dashboard infrastructure (50+ clients)
- Each client needs automated data refresh (daily/weekly/monthly)
- Scheduled pipelines, testing workflows, deployment triggers
- Cross-repo coordination

::: {.fragment}
**The Old Response:**

"This would take weeks to set up properly. I'd need to learn GitHub Actions deeply."
:::

::: {.fragment}
**With Claude Code:**

Built comprehensive data orchestration system in days, not weeks.
:::

::: {.notes}
This is a real example from my client work. They needed an enterprise-grade data orchestration system to manage 50+ client dashboards, each with different refresh schedules.

**The Challenge:** GitHub Actions YAML syntax is finicky. Scheduled workflows, manual testing pipelines, repository dispatch events for cross-repo triggers, configuration-driven scheduling‚Äîthis is DevOps territory.

**My Approach:** I understood the architecture (what needed to happen and when), but I didn't want to spend weeks becoming a GitHub Actions expert.

**Claude's Role:** Generated the YAML workflows, helped set up the orchestrator pattern, created the manual testing workflows, and built the configuration-driven scheduling system.

**My Role:** Designed the overall architecture, specified requirements, tested everything, and ensured it worked for the business use case.

**Result:** Production-ready system that scales to 50+ clients, all automated, all documented. I now understand GitHub Actions well enough to maintain and extend it, but I didn't have to become an expert to deliver value.
:::

---

## The GitHub Actions YAML {.smaller}

```yaml
name: Data Orchestrator
on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:

jobs:
  orchestrate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup R
        uses: r-lib/actions/setup-r@v2

      - name: Process clients based on schedule
        run: |
          Rscript scripts/orchestrator.R
        env:
          GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
```

::: {.fragment}
**I understand what it does. Claude wrote the syntax.**
:::

::: {.notes}
This is a snippet of actual code from a client data orchestration system. Look at what's happening here:

**Scheduled trigger:** Runs daily at 6 AM UTC, processes all clients based on their individual refresh schedules (daily, weekly, monthly, quarterly).

**Manual trigger:** `workflow_dispatch` allows testing on demand.

**R integration:** Sets up R environment, installs packages, runs the orchestrator script.

**Secrets management:** Properly uses GitHub secrets for cross-repo operations and database access.

**The key insight:** I didn't need to memorize GitHub Actions syntax. I needed to understand the architecture‚Äîwhat needs to happen, when, and why. Claude Code handled translating that understanding into correct YAML.

This is "staying in your R lane while extending your reach." The core logic is R code (my expertise). The automation wrapper is GitHub Actions (Claude's expertise).
:::

---

## Client Project Workflow: The Pattern {.smaller}

::: {.incremental}
1. **Understand Requirements** (Human) ‚Üí Define the problem and technical approach
2. **Generate Implementation** (Claude Code) ‚Üí Write the initial code
3. **Review & Course-Correct** (Human) ‚Üí Read every line, check for issues
4. **Iterate Based on Feedback** (Claude Code) ‚Üí Fix problems, refine solution
5. **Final QA & Delivery** (Human) ‚Üí Test thoroughly and ship to client
:::

::: {.fragment}
**Notice the pattern:** Human decisions at the start and end. AI execution in the middle.
:::

::: {.notes}
This is my actual workflow for every client project. Let me walk through what this looks like in practice:

**1. Understand Requirements (Human):** I talk to the client, understand their business problem, and translate that into technical requirements. This is where domain expertise matters‚Äîknowing what questions to ask, understanding their constraints.

**2. Generate Implementation (Claude Code):** I describe what I need to Claude. "Build a Shiny module that handles user authentication." "Create a GitHub Actions workflow for automated testing." Claude generates the initial implementation.

**3. Review & Course-Correct (Human):** This is critical. I read every line of generated code. Does it match my mental model? Are there security issues? Does it follow R best practices? Does it fit the client's existing architecture?

**4. Iterate Based on Feedback (Claude Code):** When I find issues (and I always do), I direct Claude to fix them. "This function should use dplyr instead of base R." "Add error handling for API failures."

**5. Final QA & Delivery (Human):** I test everything thoroughly, write documentation, and deliver to the client. This is where my reputation is on the line‚Äîthe final quality check is always human.

The pattern ensures that strategic decisions and quality control stay with the human, while implementation speed comes from AI assistance.
:::

---

## Tips & Tricks: Managing Your AI Partner {.smaller}

::: {.incremental}
**Be Specific with Requirements**

- "Add error handling" ‚Üí "Add tryCatch with informative error messages for API failures"
- "Make it faster" ‚Üí "Use data.table instead of dplyr for this 1M+ row operation"

**Always Review Generated Code Before Running**

- Check for security issues (hardcoded credentials, SQL injection risks)
- Verify package dependencies actually exist
- Ensure code follows your team's style guide

**Use Incremental Commits to Track What Worked**

- Commit after each successful change
- Makes it easy to roll back when Claude goes off track
- Detailed commit messages help future you understand decisions

**Keep Claude Focused on R/Your Core Stack**

- Don't let it convince you to rewrite everything in Python
- Stay in your lane‚Äîuse AI to extend reach, not replace your expertise
:::

::: {.notes}
These are hard-won lessons from daily use of Claude Code in client work.

**Be Specific:** Vague instructions get vague results. The more specific you are about what you want, the better Claude's output will be. Think of it like managing a junior developer‚Äîclear requirements lead to better code.

**Always Review:** This cannot be emphasized enough. Claude will confidently generate code that looks right but has subtle bugs. It will hallucinate package functions. It will suggest approaches that work but aren't appropriate for your use case. Your job is to catch these issues.

**Incremental Commits:** This is a game-changer. When you commit frequently, you can easily see what changed and why. When Claude suggests a refactoring that breaks everything, you can quickly revert. Plus, it creates an audit trail of your decision-making.

**Stay in Your Lane:** Claude will sometimes suggest using different languages or frameworks. "This would be easier in Python." Maybe, but if your expertise is R and your client expects R code, stay in R. Use Claude to get better at R, not to escape it.

The goal is to amplify your existing skills, not replace them.
:::

---

## When to Use Claude Code vs. Code Yourself {.smaller}

::: columns
::: {.column width="50%"}
**Use Claude Code for:**

::: {.incremental}
- Boilerplate and scaffolding
- Unfamiliar packages/syntax
- Repetitive transformations
- Test data generation
- Documentation writing
- YAML/config files
:::
:::

::: {.column width="50%"}
**Code Yourself for:**

::: {.incremental}
- Business logic
- Client-specific domain knowledge
- Performance-critical sections
- Security-sensitive code
- Final architecture decisions
:::
:::
:::

::: {.fragment}
<div style="text-align: center; margin-top: 20px;">
**The sweet spot:** Use Claude for the "how" while you own the "what" and "why."
</div>
:::

::: {.notes}
This is the decision framework I use every day. Let me explain the reasoning:

**Use Claude Code for:**
- **Boilerplate:** Setting up Shiny modules, package structure, GitHub Actions templates‚Äîthese follow known patterns. Let Claude handle it.
- **Unfamiliar packages:** When I need to use a package I've never used before, Claude can generate example code faster than I can read documentation.
- **Repetitive transformations:** If I'm doing the same data manipulation 20 times with different columns, Claude can generate the variations.
- **Test data:** Creating realistic fake data is tedious. Claude excels at this.
- **Documentation:** Claude can draft README files and function documentation. I review and refine.
- **YAML/config:** These are finicky and tedious. Perfect for AI assistance.

**Code Yourself for:**
- **Business logic:** The core algorithms that make your solution valuable‚Äîthese need human understanding.
- **Client-specific domain knowledge:** Claude doesn't know your client's business. You do.
- **Performance-critical sections:** When milliseconds matter, you need to understand exactly what's happening.
- **Security-sensitive code:** Authentication, authorization, data encryption‚Äîreview these extremely carefully or write them yourself.
- **Architecture decisions:** Should this be a Shiny module or a function? Should we use Supabase or PostgreSQL? These strategic decisions are yours.

The key insight: Claude Code is best for the "how" (implementation details), while you should always own the "what" (requirements) and "why" (strategic decisions).
:::

---

## The Next Frontier: Subagents {.smaller}

::: {.fragment}
**Coming soon to Claude Code:** Specialized agents for specific tasks
:::

::: {.incremental}
- **Testing agent:** Automatically write unit tests for new functions
- **Documentation agent:** Generate comprehensive docs from code
- **Deployment agent:** Handle CI/CD and infrastructure as code
- **Code review agent:** Catch bugs and suggest improvements
:::

::: {.fragment}
<div style="text-align: center; margin-top: 20px;">
**The future:** AI partners that handle entire workflows, not just individual tasks.
</div>
:::

::: {.notes}
This is a teaser for where AI-assisted development is heading. I'm currently exploring subagents in Claude Code, and it's genuinely exciting.

**What are subagents?** Specialized AI agents that focus on specific types of tasks. Instead of one general-purpose AI assistant, you have a team of specialists.

**Testing agent:** When you write a new function, the testing agent can automatically generate unit tests that cover edge cases you might not have thought of.

**Documentation agent:** Analyzes your code and generates comprehensive documentation‚Äînot just function signatures, but usage examples and explanations.

**Deployment agent:** Handles the DevOps work‚Äîwriting Dockerfiles, setting up CI/CD pipelines, managing infrastructure as code.

**Code review agent:** Acts like a senior developer reviewing your code‚Äîcatches potential bugs, suggests performance improvements, identifies security issues.

**Why this matters:** Right now, I manage Claude Code task by task. With subagents, I could describe a feature and let specialized agents handle testing, documentation, and deployment automatically.

**The caveat:** I'm still exploring this. The human-in-the-loop principle remains‚Äîyou're still the manager, making strategic decisions and ensuring quality. But the AI team gets more capable.

This is the next evolution of AI-assisted development, and I'm excited to see where it goes.
:::


# Part 3: Building Up Your R Intuition Through Volume

## Learning Acceleration

---

## The Traditional Learning Curve

```{r}
#| echo: true
#| eval: false

# Learning new R patterns traditionally:
# 1. Read documentation (30 minutes)
# 2. Try implementation (1 hour)
# 3. Hit error (5 minutes of frustration)
# 4. Google/Stack Overflow (20 minutes)
# 5. Try again (30 minutes)
# 6. Success! (Maybe)
#
# Total time: 2.5+ hours for one pattern

learning_rate_traditional <- "Slow but deliberate"
```

---

## The High-Volume Approach

```{r}
#| echo: true
#| eval: false

# Learning with AI assistance:
# 1. Ask Claude for implementation (2 minutes)
# 2. Review generated code (5 minutes)
# 3. Test it (5 minutes)
# 4. Encounter error (immediate)
# 5. Understand WHY it failed (learning moment)
# 6. Adjust and iterate (10 minutes)
#
# Total time: 22 minutes
# But you've seen 3-4 different approaches

learning_rate_ai_assisted <- "Fast iteration builds intuition"
```

---

## Practical Use Cases {.smaller}

**1. Generating Fake Data**

```{r}
#| echo: true
#| eval: false

# "Claude, generate realistic university course enrollment data"
set.seed(123)
courses <- data.frame(
  course_id = paste0("COURSE", 1:50),
  department = sample(c("Math", "CS", "Biology", "English", "History"), 50, replace = TRUE),
  enrollment = sample(15:150, 50, replace = TRUE),
  semester = sample(c("Fall 2024", "Spring 2025"), 50, replace = TRUE),
  instructor_rating = round(runif(50, 2.5, 5.0), 1)
)

# This would have taken me 20+ minutes
# Claude did it in 30 seconds
# I learned about realistic data generation patterns
```

---

## Practical Use Cases {.smaller}

**2. Project Scaffolding**

```{r}
#| echo: true
#| eval: false

# "Claude, create a basic Shiny app structure with modules"

# app.R
library(shiny)
source("modules/data_module.R")
source("modules/plot_module.R")

ui <- fluidPage(
  titlePanel("Dashboard"),
  dataModuleUI("data"),
  plotModuleUI("plot")
)

server <- function(input, output, session) {
  data <- dataModuleServer("data")
  plotModuleServer("plot", data)
}

shinyApp(ui, server)

# The "getting started" friction is eliminated
```

---

## Learning from Claude's Real Mistakes {.smaller}

**Claude makes mistakes. This is actually valuable‚Äîhere are real examples from building this presentation:**

::: {.incremental}
**1. The SHINYFA Hallucination**

- Asked Claude to write content about the `{shinyfa}` package
- Claude confidently said it was for "Font Awesome icons in Shiny apps"
- Generated example code: `fa_icon("rocket", fill = "#0051BA")`
- **Reality:** {shinyfa} analyzes Shiny app reactivity patterns (completely different!)
- **Fix:** Provided the actual documentation URL, Claude self-corrected
- **Lesson:** Always verify domain-specific claims, especially for packages

**2. The Column Name Disaster**

- Trying to create bird diversity plots with `{avilistr}` package
- Claude used `avilist_global` dataset (doesn't exist‚Äîit's `avilist_2025`)
- Used lowercase `order_name` column (actual column is `Order`, capitalized)
- Multiple rounds of "Column not found" errors
- **Fix:** Checked the actual package structure: `names(avilist_2025)`
- **Lesson:** When stuck in error loops, inspect the source directly

**3. The Package Function Hunt**

- Wanted a Sankey chart for mental bandwidth visualization
- Claude tried `make_long()` from ggsankey (function doesn't exist)
- Tried `sankeyfy()` (also doesn't exist in that package)
- Multiple package attempts before admitting defeat
- **Fix:** Pivoted to HTML/CSS solution that actually worked
- **Lesson:** Be ready to abandon AI's approach when it's not working
:::

::: {.fragment}
**These mistakes made me a better R developer‚ÄîI had to understand the packages deeply to fix them.**
:::

::: {.notes}
These are three real examples from building this very presentation. Each mistake was a learning opportunity that improved my R skills.

**The SHINYFA Hallucination:** This is documented in the CLAUDE.md file. When I asked Claude to create presentation content about my {shinyfa} package, it completely fabricated what the package does. It assumed from the name that "shinyfa" must be about Font Awesome icons and generated confident, plausible-sounding (but completely wrong) example code. I caught it because I wrote the package‚ÄîI know what it actually does. This taught me to always verify AI-generated content about domain-specific tools, especially in the R ecosystem where package names can be misleading.

**The Column Name Disaster:** This happened while creating the {avilistr} examples. Claude used the wrong dataset name (`avilist_global` instead of `avilist_2025`) and wrong column names (`order_name` instead of `Order`). We went through multiple error cycles before I stopped and actually checked the package structure myself using `names(avilist_2025)`. This forced me to learn how to properly inspect package datasets, check column names, and understand the data structure‚Äîskills I use constantly now.

**The Package Function Hunt:** When creating the mental bandwidth visualization, Claude tried multiple R packages for Sankey charts. It confidently used functions like `make_long()` and `sankeyfy()` that don't actually exist (or don't work as expected). After several failed attempts, I made the call to pivot to an HTML/CSS solution. This taught me that sometimes the "pure R" solution isn't the best solution, and it's okay to use other tools when they're more appropriate.

**The key insight:** Each of these mistakes forced me to develop deeper understanding. I couldn't just accept Claude's output‚ÄîI had to understand the packages, data structures, and available functions well enough to correct the errors. This is exactly the kind of learning that builds intuition and makes you a better developer. You're not becoming dependent on AI; you're accelerating your learning by encountering (and fixing) more problems faster than traditional development would expose you to.
:::

---

## The Failing Fast Advantage

```{r}
#| echo: true
#| eval: false

# Traditional development:
attempts <- 1
learning <- "slow and careful"

# AI-assisted development:
attempts <- 10
learning <- "rapid pattern recognition"

# You encounter and solve more problems in less time
# Each failure teaches you something about:
# - What works in R
# - What's idiomatic
# - What's maintainable
# - What's efficient
```

---

## The 80% Problem: AI Limitations {.smaller}

**Not everything AI generates is production-ready.**

::: columns
::: {.column width="50%"}
**Where AI Excels:**

- Boilerplate code
- Standard patterns
- Initial structure
- Quick prototypes
- Documentation drafts
:::

::: {.column width="50%"}
**Where AI Falls Short:**

- Data cleaning scripts (‚âà80% okay)
- Edge case handling
- Domain-specific logic
- Performance optimization
- Security considerations
:::
:::

<br>

::: {.fragment}
**The term:** "AI Slop" - code that *technically works* but isn't quite right.
:::

---

## Over-Abstraction Warning

```{r}
#| echo: true
#| eval: false

# Claude's suggestion (over-abstracted):
create_advanced_pipeline <- function(data, config, transformations, validators,
                                      error_handlers, logging_strategy, ...) {
  # 200 lines of abstraction
}

# What I actually need (maintainable):
clean_data <- function(data) {
  data |>
    filter(!is.na(key_column)) |>
    mutate(date = as.Date(date))
}

# Keep it simple. Keep it readable. Keep it maintainable.
```

---

# Part 4: The Human in the Loop

## Maintaining Technical Leadership

---

## Establishing Your AI Values

Before integrating AI into your workflow, ask yourself:

::: {.incremental}
1. **What code can I maintain 6 months from now?**
2. **Where do I need to maintain expertise?**
3. **What represents my core value proposition?**
4. **What should I always understand deeply?**
:::

::: {.fragment}
**These become your guardrails.**
:::

---

## Defining the Relationship

**Claude is not your boss. Claude is your colleague.**

::: columns
::: {.column width="50%"}
**Claude as Colleague:**

- Suggests approaches
- Generates initial drafts
- Handles boilerplate
- Explores alternatives
- Speeds up research
:::

::: {.column width="50%"}
**You as Leader:**

- Make architectural decisions
- Review and refine code
- Ensure maintainability
- Apply domain expertise
- Own the final product
:::
:::

---

## The Dumpster Fire Prevention Rule

**If you can't explain the code, you don't own it.**

```{r}
#| echo: true
#| eval: false

# Bad: AI-generated code you don't understand
result <- complex_ai_function(data, params = list(
  algorithm = "advanced_ml_technique",
  hyperparameters = list(alpha = 0.05, beta = 0.9, gamma = 2.5),
  cross_validation = TRUE,
  ensemble_method = "stacking"
))

# Good: Code you understand and can maintain
result <- data |>
  filter(status == "active") |>
  group_by(category) |>
  summarize(total = sum(value))

# If you can't debug it, don't ship it.
```

---

## Example 4: Quarto Over JavaScript {.smaller}

**The Scenario:**

- Client wants an interactive dashboard
- AI suggests a JavaScript framework (React, Vue, etc.)
- Impressive demo, cutting-edge tech

**The Problem:**

- I'm an R developer, not a JavaScript expert
- Client needs updates and maintenance
- In 6 months, can I confidently modify this code?

---

## Staying in Your Lane

```{r}
#| echo: true
#| eval: false

# Instead of AI-generated JavaScript:
# ‚ùå Complex build process
# ‚ùå Node dependencies
# ‚ùå Framework-specific patterns I don't deeply understand

# I chose Quarto:
# ‚úÖ Markdown-based (familiar)
# ‚úÖ R code chunks (my expertise)
# ‚úÖ Easy to maintain
# ‚úÖ Client can understand the structure

library(ggplot2)
library(dplyr)

# This is my wheelhouse
# I can modify this at 2am if needed
# The client can read this
```

---

## Hand on the Wheel

**Key Principle:** AI accelerates; you steer.

::: {.incremental}
- **Let AI generate** the boilerplate, the repetitive code, the initial structure
- **You maintain** the architecture, the business logic, the critical decisions
- **Review everything** like you're mentoring an intern
- **Refactor freely** to match your style and standards
- **Test thoroughly** because AI doesn't understand your edge cases
:::

---

## When to Override AI

You should override or ignore AI suggestions when:

1. The solution is over-engineered for your needs
2. It uses technologies outside your maintenance capacity
3. It introduces dependencies you don't want to manage
4. You can't explain it to your client
5. It feels like "magic" instead of logic

**Your expertise is your compass.**

---

# Conclusion

## Bringing It All Together

---

## The Four Pillars

::: columns
::: {.column width="50%"}
**1. Mental Bandwidth**

- 10-20% time back
- Strategic focus
- Creative projects

**2. Bigger Problems**

- Say yes to growth
- Expand service offerings
- Comprehensive solutions
:::

::: {.column width="50%"}
**3. Accelerated Learning**

- High-volume iteration
- Pattern recognition
- Fail fast, learn faster

**4. Human Leadership**

- Establish AI values
- Maintain expertise
- Own the final product
:::
:::

---

## The Core Philosophy

**AI doesn't replace expertise.**

**It amplifies it.**

<br>

::: {.incremental}
- You remain the architect
- You maintain the standards
- You own the relationships
- You deliver the value
:::

---

## Your AI Values Exercise

Before your next AI-assisted project, write down:

1. **What I will always do myself:**
2. **What AI can help me with:**
3. **What I need to understand deeply:**
4. **What I'm comfortable delegating:**

**This clarity prevents the dumpster fire.**

---

## Final Thoughts {.center}

> "When AI frees up your mental bandwidth,
> you don't get lazy‚Äîyou get strategic."

<br>

**Stay in your lane.**

**Keep your hand on the wheel.**

**Build with confidence.**

---

## Thank You {.center}

![](images/horizontal-logo.png){width=60% fig-align="center"}

**Questions?**

<br>

Contact: [jasmine@dalyanalytics.com]

GitHub: [@jasdumas]

Website: [dalyanalytics.com]

::: {.notes}
Thank you all for your time. I'm happy to take questions about AI-assisted development, R workflows, or anything else we've covered today.
:::
